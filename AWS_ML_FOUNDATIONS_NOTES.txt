A machine learning model is a block of code or framework that can be modified to solve different but related problems
based on the data provided.

A model is an extremely generic program, made specific by the data used to train it.

Model training algorithms work through an interactive process where the current model iteration is analyzed to
determine what changes can be made to get closer to the goal. Those changes are made and the iteration continues until the model is evaluated to meet the goals.

Model inference is when the trained model is used to generate predictions

Impute is a common term referring to different statistical tools which can be used to calculate missing values
from your dataset.

Outliers are data points that are significantly different from others in the same sample.

Model Training Terminology
The model training algorithm iteratively updates a model's parameters to minimize some loss function.

Model parameters: Model parameters are settings or configurations the training algorithm can update to change
how the model behaves.

Weights, which are values that change as the model learns, are more specific to neural networks.

Loss function: A loss function is used to codify the model’s distance from this goal.

Linear models are fast to train and give you a great baseline against which to compare more complex models.

Deep learning models
Extremely popular and powerful, deep learning is a modern approach based around a conceptual model of how
the human brain functions.

The model (also called a neural network) is composed of collections of neurons (very simple computational units)
connected together by weights (mathematical representations of how much information to allow to flow from one
neuron to the next)

FFNN: The most straightforward way of structuring a neural network, the Feed Forward Neural Network (FFNN)
structures neurons in a series of layers, with each neuron in a layer containing weights to all neurons in
the previous layer.

CNN: Convolutional Neural Networks (CNN) represent nested filters over grid-organized data.
They are by far the most commonly used type of model when processing images.

RNN/LSTM: Recurrent Neural Networks (RNN) and the related Long Short-Term Memory (LSTM) model types are
structured to effectively represent for loops in traditional computing,
collecting state while iterating over some object. They can be used for processing sequences of data.

Transformer: A more modern replacement for RNN/LSTMs, the transformer architecture enables training over larger datasets involving sequences of data.

Hyperparameters are settings on the model which are not changed during training but can affect how quickly or how
reliably the model trains, such as the number of clusters the model should identify. WHILE
Model parameters are settings or configurations the training algorithm can update to change how the model behaves.

Log loss seeks to calculate how uncertain your model is about the predictions it is generating.
In this context, uncertainty refers to how likely a model thinks the predictions being generated are to be correct.

Model Accuracy is the fraction of predictions a model gets right.

METRICS FOR REGRESSION
Mean absolute error (MAE): This is measured by taking the average of the absolute difference between the
actual values and the predictions. Ideally, this difference is minimal.

Root mean square error (RMSE): This is similar MAE, but takes a slightly modified approach so values with
large error receive a higher penalty.
RMSE takes the square root of the average squared difference between the prediction and the actual value.
it is also called AVERAGE ERROR(and should be low)

Coefficient of determination or R-squared (R^2): This measures how well-observed outcomes are actually
predicted by the model, based on the proportion of total variation of outcomes.

Once you have trained your model, have evaluated its effectiveness, and are satisfied with the results,
you're ready to generate predictions on real-world problems using unseen data in the field.
In machine learning, this process is often called inference

Model inference involves...

Generating predictions.

Finding patterns in your data.

Using a trained model.

Testing your model on data it has not seen before.



predicting the height (label) of a thrown projectile over time (input variable). is not linear; it's curved.
Any straight line you try to use to describe this phenomenon would be invalid for a large range of the projectile's
trajectory.

Techniques do exist to modify your data so you can still use linear models in these situations.
Such methods are called kernel method

"a' "the' are referred to as STOP WORDS in ML

the silhouette coefficient is a good metric for measuring unsupervised learning(clustering) and k-means is a good
baseline model to start with... k is the no of clusters we want to find

MACHINE LEARNING GLOSSARY
1. Bag of words: A technique used to extract features from the text. It counts how many times a word appears in a
document (corpus), and then transforms that information into a dataset.

2. Data vectorization: A process that converts non-numeric data into a numerical format so that it can be used by
a machine learning model.

3. Silhouette coefficient: A score from -1 to 1 describing the clusters found during modeling. A score near zero
indicates overlapping clusters, and scores less than zero indicate data points assigned to incorrect clusters.
A score approaching 1 indicates successful identification of discrete non-overlapping clusters.

4. Stop words: A list of words removed by natural language processing tools when building your dataset.
There is no single universal list of stop words used by all-natural language processing tools.

5. CNN (convolutional neural network)
Neural networks are a collection of very simple models connected together. These simple models are called neurons,
and the connections between these models are trainable model parameters called weights.
CNN: Convolutional Neural Networks (CNN) represent nested filters over grid-organized data.
They are by far the most commonly used type of model when processing images.

Precision and Recall will be effective for image detection, spill or no spill( supervised learning).
You can think of precision as answering the question, "Of all predictions of a spill, how many were right?"
and recall as answering the question, "Of all actual spills, how many did we detect?"

6. A categorical label has a discrete set of possible values, such as "is a cat" and "is not a cat."

7. Clustering-Unsupervised learning task that helps to determine if there are any naturally occurring groupings
in the data.

8. A continuous (regression) label does not have a discrete set of possible values, which means possibly an
unlimited number of possibilities.

9. Data vectorization: A process that converts non-numeric data into a numerical format so that it can be
used by a machine learning model.

10. Discrete: A term taken from statistics referring to an outcome taking on only a finite number of values
(such as days of the week).

11. FFNN: The most straightforward way of structuring a neural network, the Feed Forward Neural Network (FFNN)
structures neurons in a series of layers, with each neuron in a layer containing weights to all neurons in the
previous layer.

12. Hyperparameters are settings on the model which are not changed during training but can affect how
quickly or how reliably the model trains, such as the number of clusters the model should identify.

13. Log loss is used to calculate how uncertain your model is about the predictions it is generating.

14. Hyperplane: A mathematical term for a surface that contains more than two planes.

15. Impute is a common term referring to different statistical tools which can be used to calculate missing values
 from your dataset.

16. label refers to data that already contains the solution.

17. loss function is used to codify the model’s distance from this goal

18. Machine learning, or ML, is a modern software development technique that enables computers to solve p
roblems by using examples of real-world data.

19. Model accuracy is the fraction of predictions a model gets right.

20. Discrete: A term taken from statistics referring to an outcome taking on only a finite number of values
(such as days of the week).

21. Continuous: Floating-point values with an infinite range of possible values. The opposite of categorical or
discrete values, which take on a limited number of possible values.

22. Model inference is when the trained model is used to generate predictions.

23. Model training algorithms work through an interactive process where the current model iteration is analyzed
to determine what changes can be made to get closer to the goal. Those changes are made and the iteration
continues until the model is evaluated to meet the goals.

24. Neural networks: a collection of very simple models connected together. These simple models are called neurons.
 The connections between these models are trainable model parameters called weights.

25. Outliers are data points that are significantly different from others in the same sample.

26. Plane: A mathematical term for a flat surface (like a piece of paper) on which two points can be joined by a
straight line.

27. Regression: A common task in supervised machine learning.

28. In reinforcement learning, the algorithm figures out which actions to take in a situation to maximize a
reward (in the form of a number) on the way to reaching a specific goal.

29. RNN/LSTM: Recurrent Neural Networks (RNN) and the related Long Short-Term Memory (LSTM) model types are
structured to effectively represent for loops in traditional computing, collecting state while iterating over
some object. They can be used for processing sequences of data.

30. Stop words: A list of words removed by natural language processing tools when building your dataset.
There is no single universal list of stop words used by all-natural language processing tools.

31. In supervised learning, every training sample from the dataset has a corresponding label or output value
associated with it. As a result, the algorithm learns to predict labels or output values.

32. Test dataset: The data withheld from the model during training, which is used to test how well your
model will generalize to new data.

33. Training dataset: The data on which the model will be trained. Most of your data will be here.

34. Transformer: A more modern replacement for RNN/LSTMs, the transformer architecture enables training over
larger datasets involving sequences of data.

35. In unlabeled data, you don't need to provide the model with any kind of label or solution while the model
is being trained.

36. In unsupervised learning, there are no labels for the training data. A machine learning algorithm tries to
learn the underlying patterns or distributions that govern the data.


SOFTWARE ENGINEERING PRACTICES
code should be simple, concise and readable
code should be modularized,, separated into functions and modules for reusability organization and to avoid repetition
module is  a file that helps store codes and can be imported into other files

Refactoring is restructing code to improve internal structure without changing external functionality 
refatoring code
reduces work load on the long run
makes u a better developer
reuse the code
organize and maintain the code


in writing clean code
1. use meaningful names that are descriptive and imply the datatype e,g ages_list, is_minor( for conditions(booleans))
2. avoid long names
3. avoid abbreviations and single letters
4. use whitespaces well, consistently
5. a line should be limited to 78 char(pep8 standard)

in wrting modular code
DRY-dont repeat yourself: moduarized code help to reuse part of  acode
spaghetti code is code that is repetitive
abstrct out logic into functions
minimize the no of entities(modules, functions
a function should do one thing
arbitrary names can be used in functions 
three or less arguments in a function

Efficient code
- reduce run time
-reduce space in memory

pick vector operators over loop,,, use numpy and pandas over loops

optimize codes,, use the one thT saves time to execute

Documentation: Additional text or illustrated information that comes with or is embedded in the code of software.
Documentation is helpful for clarifying complex parts of code, making your code easier to navigate, and quickly conveying how and why different components of your program are used.
Several types of documentation can be added at different levels of your program:
Inline comments - line level
Docstrings - module and function level
Project documentation - project level


Code properly prepared for industry use is tested code

Bad encoding, inappropriate and unexpected features can ruin our codes

TEST DRIVEN DEVELOPMENT
writing test before developing code to carry out tasks
Unit test is a test that covers a unto of code, usually a function independently from the rest of the program
We want to test our functions in a way that is repeatable and automated.
assert is used to check whether a result matches the required output
The advantage of unit tests is that they are isolated from the rest of your program, and thus, no dependencies are involved.

Integration testing exercises two or more parts of an application at once, including the interactions between the parts, to determine if they function as intended.

While unit testing is used to find bugs in individual functions, integration testing tests the system as a whole. These two approaches should be used together, instead of doing just one approach over the other.

pytest is a good tool for testing
create a test file with all the functions, run pytest on the terminal
name test files with test_ and functions in it with test_ for pytest to recognize it.. use one assert statemnet per function

Unit Testing Tools
To install pytest, run pip install -U pytest in your terminal. You can see more information on getting started here.

Create a test file starting with test_.
Define unit test functions that start with test_ inside the test file.
Enter pytest into your terminal in the directory of your test file and it detects these tests for you.
test_ is the default; if you wish to change this, you can learn how in this pytest configuration.

In the test output, periods represent successful unit tests and Fs represent failed unit tests. Since all you see is which test functions failed, it's wise to have only one assert statement per test. Otherwise, you won't know exactly how many tests failed or which tests failed.

Your test won't be stopped by failed assert statements, but it will stop if you have syntax errors.

Test-driven development and data science
Test-driven development: Writing tests before you write the code that’s being tested. Your test fails at first, and you know you’ve finished implementing a task when the test passes.
Tests can check for different scenarios and edge cases before you even start to write your function. When start implementing your function, you can run the test to get immediate feedback on whether it works or not as you tweak your function.
When refactoring or adding to your code, tests help you rest assured that the rest of your code didn't break while you were making those changes. Tests also helps ensure that your function behavior is repeatable, regardless of external parameters such as hardware and time.

LOGGING
Logging is valuable for understanding the events that occur while running your program.
in writing log messages
1. be professional and clear e.g this isnt working is bad.. couldnt parse file is better
2.Be concise and use normal capitalization
    Bad: Start Product Recommendation Process
    Good: Generating product recommendations.
3.Choose the appropriate level for logging
    Debug: Use this level for anything that happens in the program.
    Error: Use this level to record any error that occurs. 
    Info: Use this level to record all actions that are user driven or system specific, such as regularly scheduled operations.
    
4.Provide any useful information

Code reviews benefit everyone in a team to promote best programming practices and prepare code for production
it helps catch errors
ensure readability
check standards are met'
share knowledge among teams


Questions for a Code Review
1. Is the code clean and modular?

Can I understand the code easily?
Does it use meaningful names and whitespace?
Is there duplicated code?
Can I provide another layer of abstraction?
Is each function and module necessary?
Is each function or module too long?

2. Is the code efficient?

Are there loops or other steps I can vectorize?
Can I use better data structures to optimize any steps?
Can I shorten the number of calculations needed for any steps?
Can I use generators or multiprocessing to optimize any steps?

3. Is the documentation effective?

Are inline comments concise and meaningful?
Is there complex code that's missing documentation?
Do functions use effective docstrings?
Is the necessary project documentation provided?

4. Is the code well tested?

Does the code high test coverage?
Do tests check for interesting cases?
Are the tests readable?
Can the tests be made more efficient?

5. Is the logging effective?

Are log messages clear, concise, and professional?
Do they include all relevant and useful information?
Do they use the appropriate logging level?

OBJECT ORIENTED PROGRAMMING
class object method attributes
changing values with a method ensures flexibility in the long run, compared to changing it directly

A get method is for obtaining an attribute value. A set method is for changing an attribute value. 

A m0dule is a python file containing functions, classes and or global variables
mpdules are mpodular i.e/ canbe reused in diff places
package is a collection of modules in a directory


Python packages take advantage of object-oriented programming for a few reasons:

Object-oriented programs are relatively easy to expand, especially because of inheritance.
Object-oriented programs obscure functionality from the user. Consider scipy packages. You don't need to know how the actual code works in order to use its classes and methods.

A virtual environment is a silo-ed Python installation apart from your main Python installation

Conda does two things: manages packages and manages environments.

As a package manager, conda makes it easy to install Python packages, especially for data science. 

pip can only manage Python packages, whereas conda is a language agnostic package manager. In fact, conda was invented because pip could not handle data science packages that depended on libraries outside of Python. If you look at the history of conda, you'll find that the software engineers behind conda needed a way to manage data science packages (such as NumPy and Matplotlib) that relied on libraries outside of Python.

conda manages environments and packages. pip only manages packages.

Mixins take various forms depending on the language, but at the end of the day they encapsulate behavior that can be reused in other classes.

Pypi is a repo for python packages
it has test and regular repo.. test to test the package before uploading ti the regular for anyone to have access
license.txt contains copywright info
readme to doc how ur python work
setup.py contains info such as name of author, version of package, name of package....zip_safe=false meaning package cant be run on a zip file
